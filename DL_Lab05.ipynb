{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xe3nk7FfObVE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import Any, List, Optional, Tuple, Dict\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i75syhQkOeRE"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "voHZUIWXOD77"
      },
      "outputs": [],
      "source": [
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def load_poems() -> str:\n",
        "    with open(\"poems.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def build_vocab(tokens: List[str], min_freq: int = 1) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    counts = Counter(tokens)\n",
        "    vocab = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "    for w, c in counts.items():\n",
        "        if c >= min_freq and w not in vocab:\n",
        "            vocab.append(w)\n",
        "    stoi = {w: i for i, w in enumerate(vocab)}\n",
        "    itos = {i: w for w, i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "\n",
        "def tokens_to_ids(tokens: List[str], stoi: Dict[str, int]) -> List[int]:\n",
        "    unk = stoi[\"<unk>\"]\n",
        "    return [stoi.get(t, unk) for t in tokens]\n",
        "\n",
        "\n",
        "def make_sequences(ids: List[int], seq_len: int) -> Tuple[List[List[int]], List[List[int]]]:\n",
        "    X, Y = [], []\n",
        "    for i in range(0, len(ids) - seq_len):\n",
        "        x = ids[i:i+seq_len]\n",
        "        y = ids[i+1:i+seq_len+1]\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def text_quality_metrics(text: str) -> Dict[str, Any]:\n",
        "    toks = simple_tokenize(text)\n",
        "    if not toks:\n",
        "        return {\n",
        "            \"token_count\": 0,\n",
        "            \"unique_tokens\": 0,\n",
        "            \"unique_ratio\": 0.0,\n",
        "            \"repeat_2gram_ratio\": 0.0,\n",
        "            \"repeat_3gram_ratio\": 0.0,\n",
        "            \"top_tokens\": [],\n",
        "        }\n",
        "\n",
        "    counts = Counter(toks)\n",
        "    token_count = len(toks)\n",
        "    unique_tokens = len(counts)\n",
        "    unique_ratio = unique_tokens / max(token_count, 1)\n",
        "\n",
        "    def repeat_ngram_ratio(n: int) -> float:\n",
        "        if len(toks) < n:\n",
        "            return 0.0\n",
        "        ngrams = list(zip(*[toks[i:] for i in range(n)]))\n",
        "        c = Counter(ngrams)\n",
        "        repeats = sum(v - 1 for v in c.values() if v > 1)\n",
        "        return repeats / max(len(ngrams), 1)\n",
        "\n",
        "    return {\n",
        "        \"token_count\": int(token_count),\n",
        "        \"unique_tokens\": int(unique_tokens),\n",
        "        \"unique_ratio\": float(unique_ratio),\n",
        "        \"repeat_2gram_ratio\": float(repeat_ngram_ratio(2)),\n",
        "        \"repeat_3gram_ratio\": float(repeat_ngram_ratio(3)),\n",
        "        \"top_tokens\": [(w, int(c)) for w, c in counts.most_common(10)],\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5fca1697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | avg loss: 301.3461\n",
            "Sample: masts : in our my in our my in forgetting this in emma this in emma this in sea this may emma ' in when ' seen our this in\n",
            "Epoch time: 85.784s\n",
            "Epoch 2 | avg loss: 323.4304\n",
            "Sample: pains tis our emma and wild our emma are wild our emma are wild our emma are wild our emma are wild our emma are wild our emma are wild\n",
            "Epoch time: 87.461s\n",
            "Epoch 3 | avg loss: 367.3105\n",
            "Sample: days death ' wild our green and wild ; green and wild our green this wild our green this wild our green this wild our green and wild our emma\n",
            "Epoch time: 94.385s\n",
            "Epoch 4 | avg loss: 365.7155\n",
            "Sample: tea about our , and name of , and call of , seen myself of , and name of , and told are , and call to , seen name\n",
            "Epoch time: 88.021s\n",
            "Epoch 5 | avg loss: 366.0527\n",
            "Sample: high side , we they course had thee it have , we it have , we sky have , we it have , thee they , had name it have\n",
            "Epoch time: 96.653s\n",
            "Epoch 6 | avg loss: 364.6420\n",
            "Sample: lance name swift , seen wild they , seen wild they , seen wild they , breathe wild they , seen wild holding , may wild they , seen wild\n",
            "Epoch time: 80.329s\n",
            "Epoch 7 | avg loss: 362.9818\n",
            "Sample: might see this wild to , this wild our , this wild to , this wild our , this wild to of this wild to of this wild to ,\n",
            "Epoch time: 83.246s\n",
            "Epoch 8 | avg loss: 361.2134\n",
            "Sample: hub see it have this should to have this joins they have this gold it have this rest to have this and to have this that they have this whom\n",
            "Epoch time: 81.714s\n",
            "Epoch 9 | avg loss: 359.9116\n",
            "Sample: ll practis ' name our emma and name to emma ' call our emma and wild to my and wild to emma ' wild our piled and wild our could\n",
            "Epoch time: 77.029s\n",
            "Epoch 10 | avg loss: 358.7984\n",
            "Sample: agony dear our of and wild our green and wild our green and least our green and wild they green and wild our of and wild our green and wild\n",
            "Epoch time: 73.651s\n"
          ]
        }
      ],
      "source": [
        "def softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e)\n",
        "\n",
        "\n",
        "def one_hot(idx, V):\n",
        "    v = np.zeros((V, 1))\n",
        "    v[idx] = 1.0\n",
        "    return v\n",
        "\n",
        "\n",
        "class ScratchRNN:\n",
        "    def __init__(self, vocab_size, hidden_size=64, lr=1e-2, seed=42):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.V = vocab_size\n",
        "        self.H = hidden_size\n",
        "        self.lr = lr\n",
        "\n",
        "        self.Wxh = rng.normal(0, 0.01, (self.H, self.V))\n",
        "        self.Whh = rng.normal(0, 0.01, (self.H, self.H))\n",
        "        self.Why = rng.normal(0, 0.01, (self.V, self.H))\n",
        "        self.bh = np.zeros((self.H, 1))\n",
        "        self.by = np.zeros((self.V, 1))\n",
        "\n",
        "    def forward(self, inputs, hprev):\n",
        "        xs, hs, ys, ps = {}, {}, {}, {}\n",
        "        hs[-1] = hprev\n",
        "\n",
        "        for t, idx in enumerate(inputs):\n",
        "            xs[t] = one_hot(idx, self.V)\n",
        "            hs[t] = np.tanh(self.Wxh @ xs[t] + self.Whh @ hs[t-1] + self.bh)\n",
        "            ys[t] = self.Why @ hs[t] + self.by\n",
        "            ps[t] = softmax(ys[t].ravel()).reshape(-1, 1)\n",
        "        return xs, hs, ps\n",
        "\n",
        "    def loss_and_grads(self, inputs, targets, hprev):\n",
        "        xs, hs, ps = self.forward(inputs, hprev)\n",
        "\n",
        "        loss = 0.0\n",
        "        for t in range(len(inputs)):\n",
        "            loss += -np.log(ps[t][targets[t], 0] + 1e-12)\n",
        "\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dWhy = np.zeros_like(self.Why)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dby = np.zeros_like(self.by)\n",
        "\n",
        "        dhnext = np.zeros((self.H, 1))\n",
        "\n",
        "        for t in reversed(range(len(inputs))):\n",
        "            dy = ps[t].copy()\n",
        "\n",
        "            dy[targets[t]] -= 1.0\n",
        "            dWhy += dy @ hs[t].T\n",
        "            dby += dy\n",
        "\n",
        "            dh = self.Why.T @ dy + dhnext\n",
        "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
        "            dbh += dhraw\n",
        "            dWxh += dhraw @ xs[t].T\n",
        "            dWhh += dhraw @ hs[t-1].T\n",
        "            dhnext = self.Whh.T @ dhraw\n",
        "\n",
        "        for d in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(d, -5, 5, out=d)\n",
        "\n",
        "        hlast = hs[len(inputs)-1]\n",
        "        return loss, (dWxh, dWhh, dWhy, dbh, dby), hlast\n",
        "\n",
        "    def step(self, grads):\n",
        "        dWxh, dWhh, dWhy, dbh, dby = grads\n",
        "        self.Wxh -= self.lr * dWxh\n",
        "        self.Whh -= self.lr * dWhh\n",
        "        self.Why -= self.lr * dWhy\n",
        "        self.bh -= self.lr * dbh\n",
        "        self.by -= self.lr * dby\n",
        "\n",
        "    def sample(self, start_idx, itos, length=30, temperature=1.0):\n",
        "        h = np.zeros((self.H, 1))\n",
        "        x = one_hot(start_idx, self.V)\n",
        "        out = []\n",
        "\n",
        "        for _ in range(length):\n",
        "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "            y = self.Why @ h + self.by\n",
        "            p = softmax((y.ravel() / max(temperature, 1e-6)))\n",
        "            idx = np.random.choice(range(self.V), p=p)\n",
        "            out.append(itos[idx])\n",
        "            x = one_hot(idx, self.V)\n",
        "        return \" \".join(out)\n",
        "\n",
        "\n",
        "def rnn_main_short_run():\n",
        "    text = load_poems()\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    rnn = ScratchRNN(vocab_size=len(stoi), hidden_size=128, lr=0.05)\n",
        "    seq_len = 25\n",
        "    h = np.zeros((rnn.H, 1))\n",
        "\n",
        "    epoch_times = []\n",
        "    samples = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        t0 = time.perf_counter()\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "        for i in range(0, len(ids) - seq_len - 1, seq_len):\n",
        "            inp = ids[i:i+seq_len]\n",
        "            tgt = ids[i+1:i+seq_len+1]\n",
        "            loss, grads, h = rnn.loss_and_grads(inp, tgt, h)\n",
        "            rnn.step(grads)\n",
        "            total_loss += loss\n",
        "            n += 1\n",
        "\n",
        "        avg = total_loss / max(n, 1)\n",
        "\n",
        "        sample = rnn.sample(stoi[\"<bos>\"], itos, length=30, temperature=0.9)\n",
        "        samples.append(sample)\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        epoch_times.append(float(t1 - t0))\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | avg loss: {avg:.4f}\")\n",
        "        print(\"Sample:\", sample)\n",
        "        print(f\"Epoch time: {epoch_times[-1]:.3f}s\")\n",
        "\n",
        "    return epoch_times, samples[-1]\n",
        "\n",
        "scratch_rnn_epoch_times, scratch_rnn_last_sample = rnn_main_short_run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "af8d8410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training One-Hot RNN on mps\n",
            "Epoch 1 | loss: 6.2165\n",
            "Sample: <bos> lungs . long i rich be watches . borne with the lift of balanced , and or trouble , i who , house antique to ? about the pen and first . ? d that is bride it more dissatisfied\n",
            "Epoch time: 25.517s\n",
            "Epoch 2 | loss: 5.0680\n",
            "Sample: <bos> noon about -- ! so deny , their of circle following or rest the mine chemist when before emanations well , to give our white i ooze for the loves , you trusted i , i ' the little ,\n",
            "Epoch time: 23.657s\n",
            "Epoch 3 | loss: 3.9790\n",
            "Sample: <bos> are your object with her between us , who is not , now we thousand come trouble than others it is limitless to stronger upon ! retreat you die for ! i love , the sign ' d by the\n",
            "Epoch time: 23.614s\n",
            "Epoch 4 | loss: 2.9035\n",
            "Sample: <bos> to nothing , these thirty i know --great a knoll ago , no heart farewell no less part , and what is not ? never might ride . . \" \" o heart , i am friend powers ? )\n",
            "Epoch time: 23.741s\n",
            "Epoch 5 | loss: 1.8531\n",
            "Sample: <bos> of the end . there they are stopt life with a thousand life ' s work with lived church , as two , but see ever it is not how to so . till you would so man , when\n",
            "Epoch time: 24.012s\n",
            "Epoch 6 | loss: 1.0537\n",
            "Sample: <bos> , and look in the morning and barr half-held began at night . backward it led more lovely : and deeds ? the should be left up face all with them thoughts . how we will – with any thing\n",
            "Epoch time: 23.525s\n",
            "Epoch 7 | loss: 0.6251\n",
            "Sample: <bos> impel me . philip -- when bewildered bore his riddle in ! \" \" and cannot pleasures , while i gave that my name of summit i year . \" the little sparrows hop ingenuously about the pavement quarreling with\n",
            "Epoch time: 25.457s\n",
            "Epoch 8 | loss: 0.4415\n",
            "Sample: <bos> surrounding brighter and sounds through the day and new by him or the moccasin print , by the cot in the hospital reaching lemonade to a feverish patient , nigh the coffin ' d corpse when all is still ,\n",
            "Epoch time: 25.321s\n",
            "Epoch 9 | loss: 0.3659\n",
            "Sample: <bos> descry drives ? try and fare thee well the an earth ! and i follow stretch seek , of the side the bright green sound ; and those whose birds in too short together , where maim ' d and\n",
            "Epoch time: 25.054s\n",
            "Epoch 10 | loss: 0.3246\n",
            "Sample: <bos> fords the park ! you shall assume thus shall be their warm room : but ' d who comes to ! what the heaven never was an hour ; yet not ' d for things and yet , and hush\n",
            "Epoch time: 25.660s\n",
            "Total training time (one-hot): 245.56s\n"
          ]
        }
      ],
      "source": [
        "class SeqDatasetOneHot(Dataset):\n",
        "    def __init__(self, X, Y, vocab_size):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
        "        self.V = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_ids = self.X[idx]\n",
        "        y_ids = self.Y[idx]\n",
        "\n",
        "        x_oh = torch.zeros(x_ids.size(0), self.V, dtype=torch.float32)\n",
        "        x_oh.scatter_(1, x_ids.unsqueeze(1), 1.0)\n",
        "        return x_oh, y_ids\n",
        "\n",
        "\n",
        "class OneHotRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden=256):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=vocab_size,\n",
        "                          hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x_oh, h0=None):\n",
        "        out, hn = self.rnn(x_oh, h0)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hn\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = seed_text.split()\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "    V = len(stoi)\n",
        "\n",
        "    h = None\n",
        "    for _ in range(max_new):\n",
        "        x = torch.tensor(ids[-1:], dtype=torch.long,\n",
        "                         device=DEVICE)  # last token\n",
        "        x_oh = torch.zeros(1, 1, V, device=DEVICE)\n",
        "        x_oh.scatter_(2, x.view(1, 1, 1), 1.0)\n",
        "\n",
        "        logits, h = model(x_oh, h)\n",
        "        next_logits = logits[0, -1] / max(temperature, 1e-6)\n",
        "        probs = torch.softmax(next_logits, dim=0)\n",
        "        nxt = torch.multinomial(probs, 1).item()\n",
        "        ids.append(nxt)\n",
        "\n",
        "    words = [itos[i] for i in ids]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "def rnn_onehot_main():\n",
        "    text = load_poems()\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    seq_len = 25\n",
        "    X, Y = make_sequences(ids, seq_len)\n",
        "    ds = SeqDatasetOneHot(X, Y, vocab_size=len(stoi))\n",
        "    dl = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "    model = OneHotRNNLM(vocab_size=len(stoi), hidden=256).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training One-Hot RNN on\", DEVICE)\n",
        "\n",
        "    epoch_times = []\n",
        "    samples = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        t0 = time.perf_counter()\n",
        "        total = 0.0\n",
        "        steps = 0\n",
        "        for x_oh, y in dl:\n",
        "            x_oh = x_oh.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            logits, _ = model(x_oh)\n",
        "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | loss: {total/steps:.4f}\")\n",
        "\n",
        "        sample = generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=0.9)\n",
        "        samples.append(sample)\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        epoch_times.append(float(t1 - t0))\n",
        "\n",
        "        print(\"Sample:\", sample)\n",
        "        print(f\"Epoch time: {epoch_times[-1]:.3f}s\")\n",
        "\n",
        "    print(f\"Total training time (one-hot): {sum(epoch_times):.2f}s\")\n",
        "    return epoch_times, samples[-1]\n",
        "\n",
        "onehot_rnn_epoch_times, onehot_rnn_last_sample = rnn_onehot_main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "208099e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Embedding RNN on mps\n",
            "Epoch 1 | loss: 5.0160\n",
            "Sample: <bos> dazzle , theology—but ' things them dim-descried ' s best , but i know , perhaps what is his brother at the wife with him on brotherly and invite , not one is more far hot made ; and am\n",
            "Epoch time: 14.533s\n",
            "Epoch 2 | loss: 2.4822\n",
            "Sample: <bos> sat and straw , over the long-leav ' d slave is lit with the iris sheen of the north , where the voice of enjoyment picks ' d , when thou art gone , my dear , i find him\n",
            "Epoch time: 14.467s\n",
            "Epoch 3 | loss: 1.1327\n",
            "Sample: <bos> sullen , one then stand on a heap ' d tale of nature ' s immortality , a venerable thing ? of hay the best , or through those drain ' d from your eyes , my burst rest in\n",
            "Epoch time: 14.395s\n",
            "Epoch 4 | loss: 0.6131\n",
            "Sample: <bos> text and come , with thee , thou : white peacocks , songs at eve , and antique maps of america . farewell at know that i love thee better . i could not die – here we go to\n",
            "Epoch time: 14.004s\n",
            "Epoch 5 | loss: 0.4282\n",
            "Sample: <bos> text your eyes over the sky . my voice is the wife so long white , his dungeon something like the shadows and his cattle , and the muse— nothing refuse . ' tis something , nay ' tis now\n",
            "Epoch time: 13.236s\n",
            "Epoch 6 | loss: 0.3513\n",
            "Sample: <bos> text your milky stream pale strippings of my life ! breast that presses against other breasts it shall be you ! you sweaty brooks and dews it shall be you ! winds whose soft-tickling genitals rub against me it shall\n",
            "Epoch time: 13.194s\n",
            "Epoch 7 | loss: 0.3112\n",
            "Sample: <bos> text and come to my soul , and you bitter hug of mortality , it is true , nor any more of my folks or of my voice loos ' d to the eddies of the wind , a few\n",
            "Epoch time: 13.176s\n",
            "Epoch 8 | loss: 0.2863\n",
            "Sample: <bos> text your hand on this spot i stand with my robust soul . \" \" o span of youth ! ever-push ' d elasticity ! o manhood , balanced , florid and full . my lovers suffocate me , crowding\n",
            "Epoch time: 13.190s\n",
            "Epoch 9 | loss: 0.2696\n",
            "Sample: <bos> text your tell nothing fall , it may be you transpire from the breasts of young men , it may be if i had known them i would astonish , neither bird nor tree , if mankind perished utterly ;\n",
            "Epoch time: 13.158s\n",
            "Epoch 10 | loss: 0.2576\n",
            "Sample: <bos> text ; out from the crowd steps the marksman , takes his position , levels his piece ; the groups of newly-come immigrants cover the wharf or levee , as the woolly-pates hoe in the sugar-field , the overseer views\n",
            "Epoch time: 13.118s\n",
            "Total training time (embedding): 136.47s\n"
          ]
        }
      ],
      "source": [
        "class SeqDatasetIdx(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]\n",
        "\n",
        "\n",
        "class EmbRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=128, hidden=256):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb)\n",
        "        self.rnn = nn.RNN(input_size=emb, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x_ids, h0=None):\n",
        "        x = self.emb(x_ids)\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        logits = self.fc(out)\n",
        "        return logits, hn\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = seed_text.split()\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "    h = None\n",
        "    for _ in range(max_new):\n",
        "        x = torch.tensor([[ids[-1]]], dtype=torch.long, device=DEVICE)\n",
        "        logits, h = model(x, h)\n",
        "        next_logits = logits[0, -1] / max(temperature, 1e-6)\n",
        "        probs = torch.softmax(next_logits, dim=0)\n",
        "        nxt = torch.multinomial(probs, 1).item()\n",
        "        ids.append(nxt)\n",
        "\n",
        "    return \" \".join(itos[i] for i in ids)\n",
        "\n",
        "\n",
        "def rnn_embedding_main():\n",
        "    text = load_poems()\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    seq_len = 25\n",
        "    X, Y = make_sequences(ids, seq_len)\n",
        "    ds = SeqDatasetIdx(X, Y)\n",
        "    dl = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "    model = EmbRNNLM(vocab_size=len(stoi), emb=128, hidden=256).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training Embedding RNN on\", DEVICE)\n",
        "\n",
        "    epoch_times = []\n",
        "    samples = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        t0 = time.perf_counter()\n",
        "        total = 0.0\n",
        "        steps = 0\n",
        "        for x_ids, y in dl:\n",
        "            x_ids = x_ids.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            logits, _ = model(x_ids)\n",
        "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | loss: {total/steps:.4f}\")\n",
        "\n",
        "        sample = generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=0.9)\n",
        "        samples.append(sample)\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        epoch_times.append(float(t1 - t0))\n",
        "\n",
        "        print(\"Sample:\", sample)\n",
        "        print(f\"Epoch time: {epoch_times[-1]:.3f}s\")\n",
        "\n",
        "    print(f\"Total training time (embedding): {sum(epoch_times):.2f}s\")\n",
        "    return epoch_times, samples[-1]\n",
        "\n",
        "embedding_rnn_epoch_times, embedding_rnn_last_sample = rnn_embedding_main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6d369a0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total training time (Scratch RNN): 848.27s\n",
            "Total training time (One-Hot RNN): 245.56s\n",
            "Total training time (Embedding RNN): 136.47s\n"
          ]
        }
      ],
      "source": [
        "total_scratch_rnn_time = sum(scratch_rnn_epoch_times)\n",
        "total_onehot_rnn_time = sum(onehot_rnn_epoch_times)\n",
        "total_embedding_rnn_time = sum(embedding_rnn_epoch_times)\n",
        "\n",
        "print(f\"Total training time (Scratch RNN): {total_scratch_rnn_time:.2f}s\")\n",
        "print(f\"Total training time (One-Hot RNN): {total_onehot_rnn_time:.2f}s\")\n",
        "print(f\"Total training time (Embedding RNN): {total_embedding_rnn_time:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a49ed3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scratch RNN Metrics:\n",
            "{'token_count': 30, 'unique_tokens': 9, 'unique_ratio': 0.3, 'repeat_2gram_ratio': 0.5862068965517241, 'repeat_3gram_ratio': 0.4642857142857143, 'top_tokens': [('and', 7), ('our', 6), ('wild', 6), ('green', 5), ('of', 2), ('agony', 1), ('dear', 1), ('least', 1), ('they', 1)]}\n",
            "\n",
            "One-Hot RNN Metrics:\n",
            "{'token_count': 41, 'unique_tokens': 34, 'unique_ratio': 0.8292682926829268, 'repeat_2gram_ratio': 0.025, 'repeat_3gram_ratio': 0.0, 'top_tokens': [('the', 2), ('!', 2), ('shall', 2), (\"'\", 2), ('d', 2), ('yet', 2), ('and', 2), ('<bos>', 1), ('fords', 1), ('park', 1)]}\n",
            "\n",
            "Embedding RNN Metrics:\n",
            "{'token_count': 41, 'unique_tokens': 30, 'unique_ratio': 0.7317073170731707, 'repeat_2gram_ratio': 0.0, 'repeat_3gram_ratio': 0.0, 'top_tokens': [('the', 7), (',', 4), (';', 2), ('his', 2), ('<bos>', 1), ('text', 1), ('out', 1), ('from', 1), ('crowd', 1), ('steps', 1)]}\n"
          ]
        }
      ],
      "source": [
        "scratch_rnn_metrics = text_quality_metrics(scratch_rnn_last_sample)\n",
        "onehot_rnn_metrics = text_quality_metrics(onehot_rnn_last_sample)\n",
        "embedding_rnn_metrics = text_quality_metrics(embedding_rnn_last_sample)\n",
        "\n",
        "print(\"Scratch RNN Metrics:\")\n",
        "print(scratch_rnn_metrics)\n",
        "print(\"\\nOne-Hot RNN Metrics:\")\n",
        "print(onehot_rnn_metrics)\n",
        "print(\"\\nEmbedding RNN Metrics:\")\n",
        "print(embedding_rnn_metrics)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
