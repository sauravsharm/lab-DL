{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPfwYNytgSAKeJ8vTVnGL84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauravsharm/lab-DL/blob/main/DL_assign05_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils_text.py\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Basic word-level tokenizer:\n",
        "    - lowercases\n",
        "    - keeps words and basic punctuation as separate tokens\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # separate punctuation\n",
        "    text = re.sub(r\"([.,!?;:()\\\"'])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def load_poems(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def build_vocab(tokens: List[str], min_freq: int = 1) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    counts = Counter(tokens)\n",
        "    vocab = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "    for w, c in counts.items():\n",
        "        if c >= min_freq and w not in vocab:\n",
        "            vocab.append(w)\n",
        "    stoi = {w: i for i, w in enumerate(vocab)}\n",
        "    itos = {i: w for w, i in stoi.items()}\n",
        "    return stoi, itos\n",
        "\n",
        "\n",
        "def tokens_to_ids(tokens: List[str], stoi: Dict[str, int]) -> List[int]:\n",
        "    unk = stoi[\"<unk>\"]\n",
        "    return [stoi.get(t, unk) for t in tokens]\n",
        "\n",
        "\n",
        "def make_sequences(ids: List[int], seq_len: int) -> Tuple[List[List[int]], List[List[int]]]:\n",
        "    \"\"\"\n",
        "    Make (X, Y) where Y is next-token targets.\n",
        "    \"\"\"\n",
        "    X, Y = [], []\n",
        "    for i in range(0, len(ids) - seq_len):\n",
        "        x = ids[i:i+seq_len]\n",
        "        y = ids[i+1:i+seq_len+1]\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "    return X, Y\n"
      ],
      "metadata": {
        "id": "8RM2gY8CrGb6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R1WeY7Nre1S",
        "outputId": "41dd35d7-4a6f-469a-ac2a-828c23c642fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: utils in /usr/local/lib/python3.12/dist-packages (1.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# from utils import load_poems, simple_tokenize, build_vocab, tokens_to_ids, make_sequences\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available(\n",
        ") else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "class SeqDatasetOneHot(Dataset):\n",
        "    def __init__(self, X, Y, vocab_size):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
        "        self.V = vocab_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_ids = self.X[idx]                    # [T]\n",
        "        y_ids = self.Y[idx]                    # [T]\n",
        "        # one-hot: [T, V]\n",
        "        x_oh = torch.zeros(x_ids.size(0), self.V, dtype=torch.float32)\n",
        "        x_oh.scatter_(1, x_ids.unsqueeze(1), 1.0)\n",
        "        return x_oh, y_ids\n",
        "\n",
        "\n",
        "class OneHotRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden=256):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size=vocab_size,\n",
        "                          hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x_oh, h0=None):\n",
        "        out, hn = self.rnn(x_oh, h0)        # out: [B,T,H]\n",
        "        logits = self.fc(out)               # [B,T,V]\n",
        "        return logits, hn\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = seed_text.split()\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "    V = len(stoi)\n",
        "\n",
        "    h = None\n",
        "    for _ in range(max_new):\n",
        "        x = torch.tensor(ids[-1:], dtype=torch.long,\n",
        "                         device=DEVICE)  # last token\n",
        "        x_oh = torch.zeros(1, 1, V, device=DEVICE)\n",
        "        x_oh.scatter_(2, x.view(1, 1, 1), 1.0)\n",
        "\n",
        "        logits, h = model(x_oh, h)\n",
        "        next_logits = logits[0, -1] / max(temperature, 1e-6)\n",
        "        probs = torch.softmax(next_logits, dim=0)\n",
        "        nxt = torch.multinomial(probs, 1).item()\n",
        "        ids.append(nxt)\n",
        "\n",
        "    words = [itos[i] for i in ids]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "def main():\n",
        "    text = load_poems(\"poems.txt\")\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    seq_len = 25\n",
        "    X, Y = make_sequences(ids, seq_len)\n",
        "    ds = SeqDatasetOneHot(X, Y, vocab_size=len(stoi))\n",
        "    dl = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "    model = OneHotRNNLM(vocab_size=len(stoi), hidden=256).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training One-Hot RNN on\", DEVICE)\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(25):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        steps = 0\n",
        "        for x_oh, y in dl:\n",
        "            x_oh = x_oh.to(DEVICE)      # [B,T,V]\n",
        "            y = y.to(DEVICE)            # [B,T]\n",
        "\n",
        "            logits, _ = model(x_oh)     # [B,T,V]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | loss: {total/steps:.4f}\")\n",
        "        print(\"Sample:\", generate(model, stoi, itos,\n",
        "              seed_text=\"<bos>\", max_new=40, temperature=0.9))\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Total training time (one-hot): {elapsed:.2f}s\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwS_PvtvrCAi",
        "outputId": "4585ea21-fb73-4ed4-e40c-d2f7b692a50b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training One-Hot RNN on cuda\n",
            "Epoch 1 | loss: 6.1549\n",
            "Sample: <bos> fastenings the retire , and other , where a sold of had , and the childhood and service , and all some , i every the ages who snake the succeed ' opposite from the leaving of deep into and\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 | loss: 5.0038\n",
            "Sample: <bos> be contradict art that fraction no spring brown , where yellow-crown greatest s his early a prepared few around like myself rubs one i’d spring , and elaborate shelf your devour as the prepared sunlight and higher at the beats\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 | loss: 3.8625\n",
            "Sample: <bos> blow whether madest for a love-chant to exactly , enwrought with tale ' look each , the moon of the moon in all great or bad a hollow done with the passage to loss , in her moon , (\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 | loss: 2.6731\n",
            "Sample: <bos> curious place they are not more after me forth . and i will translate a uniform ? . it shall see – at week in faith ? would left me rubs it shall death . have who you pass ?\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 | loss: 1.6089\n",
            "Sample: <bos> heart and yet ! guard ! i think so long ? what cannot have talking for the more of heaven . o tread , for brown well-tann out of the , they were now . my mistress , she supposed\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 | loss: 0.9043\n",
            "Sample: <bos> can scorn so listen , was strive and so so things the teacher – farewell , we little pass so last and sweet when time here is more . we what have been more than the work of the little\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 | loss: 0.5563\n",
            "Sample: <bos> cuts the steady and key— like the air that the atlantic drowse like wife or her their hand , where the brook puts out of the firs , her the gymnasium , through the curtain ' d saloon , through\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 | loss: 0.4141\n",
            "Sample: <bos> stop there they no return so long but cannot , with thee down , directions , meanwhile , prevailed such an entire contentment in the air that every naked ash , and tardy tree yet leafless , showed as if\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 | loss: 0.3527\n",
            "Sample: <bos> eve , and she us our own , i should have seen with him , and luckier . \" \" if i will answer the spirit of this , the loud and still look down some wonderful cities and poet\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 | loss: 0.3176\n",
            "Sample: <bos> cucumbers with silver-wired leaves , through the salt-lick or orange glade , or under conical firs , through the gymnasium , through the curtain ' d saloon , through the office or public hall ; pleas ' d with the\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 | loss: 0.2937\n",
            "Sample: <bos> before they gain , day ? i also say it is good to fall , battles are lost in the same spirit in which they are won . i beat and pound for the dead , i blow through my\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 | loss: 0.2770\n",
            "Sample: <bos> through every fibre of manly wheat , it shall be you ! sun so generous it shall be you ! vapors lighting and shading my face it shall be you ! you sweaty brooks and dews it shall be you\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 | loss: 0.2643\n",
            "Sample: <bos> might i listening to be know i believe a refuse to go back without feeling , have , we set out my soul part of other , but they be not unite ; and now , as broken glasses show\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 | loss: 0.2546\n",
            "Sample: <bos> payment receiving and a few idly owning , and they the wheat continually claiming . this is the city and i am one of the citizens , whatever interests the rest interests me , politics , wars , markets ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 | loss: 0.2471\n",
            "Sample: <bos> scragged limbs , walking the path worn in the grass and beat through the leaves of the brush , where the quail is whistling betwixt the woods and the wheat-lot , where the bat flies in the seventh-month eve ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 | loss: 0.2408\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 | loss: 0.2359\n",
            "Sample: <bos> , where trip-hammers cock is where the heat hatches pale-green eggs in the dented sand , where the she-whale swims with her calf and never forsakes it , where the steam-ship trails hind-ways its long pennant of smoke , where\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 | loss: 0.2319\n",
            "Sample: <bos> whispers , love-root , silk-thread , crotch and vine , my respiration and inspiration , the beating of my heart , the passing of blood and air through my lungs , the sniff of green leaves and dry leaves ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 | loss: 0.2283\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 | loss: 0.2255\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 | loss: 0.2229\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 | loss: 0.2207\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 | loss: 0.2188\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 | loss: 0.2171\n",
            "Sample: <bos> text \" o my luve ' s like a youth , blood , here few fail . thousand miles , speeding with tail ' d meteors , throwing fire-balls like the rest , carrying the crescent child that carries its\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25 | loss: 0.2156\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Total training time (one-hot): 547.75s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FofNlw-no2t",
        "outputId": "4b5ad7b4-dc0e-4777-f6fb-bffeff02831c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Embedding RNN on cuda\n",
            "Epoch 1 | loss: 5.0168\n",
            "Sample: <bos> floor-men this delightful estate , i celebrate the morning of convicts , ( your collapses with soft jerks , and twist poured was born with me at last , but two well as myself ! it seem to stand as\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 | loss: 2.4437\n",
            "Sample: <bos> thee well the lights . . and loafe there ' d while i always to earth springs . i take my place among a life ' s playmate , we watch his hip-band under its couch , and massacred them\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 | loss: 1.1032\n",
            "Sample: <bos> the park will the man or woman , and cease , and naked manifold no , where bee-hives range on a bank lounged the bayou ; the winds that the heart ? when i hear you , stallion , why\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 | loss: 0.6023\n",
            "Sample: <bos> the boy i love , this is the city one will and about no more . \" \" the gray sea then , and went for a helmless as , this the meat on a summit , distant a short\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 | loss: 0.4241\n",
            "Sample: <bos> text help for one must lead to shut me , here one will be loosen ' d . earth and beat , shifting the man they are with the there , if of the vitreous pour of the full moon\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 | loss: 0.3490\n",
            "Sample: <bos> text its summer high in cars , he gives ' d ! embody all presences outlaw ' d or suffering , see myself in prison shaped like another man , and feel the dull unintermitted pain . for me the\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 | loss: 0.3094\n",
            "Sample: <bos> text that place i love him , and greater to be : ' tis now we ' re tired , my heart and i . iii . how tired we feel , my heart and i ! we seem of\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 | loss: 0.2851\n",
            "Sample: <bos> text her voluptuous limbs up her love-torch . a most gentle maid who dwelleth in her hospitable home hard by the castle , and at latest eve , ( even like a lady vow ' d and dedicate to something\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 | loss: 0.2686\n",
            "Sample: <bos> text \" \" this is the meal equally set , this the meat for natural hunger , it is for the wicked just same as the righteous , i make appointments with all , i will not have a single\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 | loss: 0.2568\n",
            "Sample: <bos> text \" o gift of god ! o perfect day : whereon shall no man work , but play ; whereon it is enough for me , not to be doing , but to be ! through every fibre of\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 | loss: 0.2482\n",
            "Sample: <bos> text \" o idle of every fibre of my brain , through every nerve , through every vein , i feel the electric thrill , the touch of life , that seems almost too much . i hear the wind\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 | loss: 0.2415\n",
            "Sample: <bos> text \" o span of youth ! ever-push ' d elasticity ! o manhood , balanced , florid and full . my lovers suffocate me , crowding my lips , thick in the pores of my skin , jostling me\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 | loss: 0.2363\n",
            "Sample: <bos> text \" o voluptuous cool-breath ' d , and cries , ‘thou shalt not be the fool of loss . ’ \" \" i sometimes hold it half a sin to put in words the grief i feel ; for\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 | loss: 0.2320\n",
            "Sample: <bos> text \" o perfect day : whereon shall no man work , but play ; whereon it is enough for me , not to be doing , but to be ! through every fibre of my brain , through every\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 | loss: 0.2285\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 | loss: 0.2255\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as they pass , because\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 | loss: 0.2233\n",
            "Sample: <bos> text \" \" shall i compare thee to a summer ' s day ? thou art more lovely and more temperate : rough winds do shake the darling buds of may , and summer ' s lease hath all too\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 | loss: 0.2211\n",
            "Sample: <bos> text \" o my luve ' s like a red , the sun and rest the chuff of your hand on my hip , and in due time you shall repay the same service to me , for after we\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 | loss: 0.2195\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve '\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 | loss: 0.2180\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 | loss: 0.2169\n",
            "Sample: <bos> text \" o my luve ' s like a red , the crowd laugh at her blackguard oaths , the men jeer and wink to each other , ( miserable ! i do not laugh at your oaths nor jeer\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 | loss: 0.2158\n",
            "Sample: <bos> text say so good will , scattering it freely forever . \" \" the pure contralto sings in the organ loft , the carpenter dresses his plank , the tongue of his foreplane whistles its wild ascending lisp , the\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 | loss: 0.2146\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s sweetly not no , but i am handcuff ' d to him and walk by his side , ( i am less the jolly one there\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 | loss: 0.2137\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25 | loss: 0.2131\n",
            "Sample: <bos> text \" o my luve ' s like a red , red rose that’s newly sprung in june ; o my luve ' s like the melodie that’s sweetly play ' d in tune . as fair art thou ,\n",
            "--------------------------------------------------------------------------------\n",
            "Total training time (embedding): 84.58s\n"
          ]
        }
      ],
      "source": [
        "# train_torch_embedding.py\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# from utils import load_poems, simple_tokenize, build_vocab, tokens_to_ids, make_sequences\n",
        "\n",
        "DEVICE = \"mps\" if torch.backends.mps.is_available(\n",
        ") else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "class SeqDatasetIdx(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.Y = torch.tensor(Y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.Y[idx]     # both [T]\n",
        "\n",
        "\n",
        "class EmbRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb=128, hidden=256):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb)\n",
        "        self.rnn = nn.RNN(input_size=emb, hidden_size=hidden, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden, vocab_size)\n",
        "\n",
        "    def forward(self, x_ids, h0=None):\n",
        "        x = self.emb(x_ids)         # [B,T,E]\n",
        "        out, hn = self.rnn(x, h0)   # [B,T,H]\n",
        "        logits = self.fc(out)       # [B,T,V]\n",
        "        return logits, hn\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, stoi, itos, seed_text=\"<bos>\", max_new=40, temperature=1.0):\n",
        "    model.eval()\n",
        "    tokens = seed_text.split()\n",
        "    ids = [stoi.get(t, stoi[\"<unk>\"]) for t in tokens]\n",
        "\n",
        "    h = None\n",
        "    for _ in range(max_new):\n",
        "        x = torch.tensor([[ids[-1]]], dtype=torch.long, device=DEVICE)  # [1,1]\n",
        "        logits, h = model(x, h)  # logits: [1,1,V]\n",
        "        next_logits = logits[0, -1] / max(temperature, 1e-6)\n",
        "        probs = torch.softmax(next_logits, dim=0)\n",
        "        nxt = torch.multinomial(probs, 1).item()\n",
        "        ids.append(nxt)\n",
        "\n",
        "    return \" \".join(itos[i] for i in ids)\n",
        "\n",
        "\n",
        "def main():\n",
        "    text = load_poems(\"poems.txt\")\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    seq_len = 25\n",
        "    X, Y = make_sequences(ids, seq_len)\n",
        "    ds = SeqDatasetIdx(X, Y)\n",
        "    dl = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "    model = EmbRNNLM(vocab_size=len(stoi), emb=128, hidden=256).to(DEVICE)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(\"Training Embedding RNN on\", DEVICE)\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(25):\n",
        "        model.train()\n",
        "        total = 0.0\n",
        "        steps = 0\n",
        "        for x_ids, y in dl:\n",
        "            x_ids = x_ids.to(DEVICE)  # [B,T]\n",
        "            y = y.to(DEVICE)          # [B,T]\n",
        "\n",
        "            logits, _ = model(x_ids)  # [B,T,V]\n",
        "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "\n",
        "            total += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | loss: {total/steps:.4f}\")\n",
        "        print(\"Sample:\", generate(model, stoi, itos,\n",
        "              seed_text=\"<bos>\", max_new=40, temperature=0.9))\n",
        "        print(\"-\"*80)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Total training time (embedding): {elapsed:.2f}s\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scratch_rnn_numpy.py\n",
        "import numpy as np\n",
        "# from utils import load_poems, simple_tokenize, build_vocab, tokens_to_ids\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e)\n",
        "\n",
        "\n",
        "def one_hot(idx, V):\n",
        "    v = np.zeros((V, 1))\n",
        "    v[idx] = 1.0\n",
        "    return v\n",
        "\n",
        "\n",
        "class ScratchRNN:\n",
        "    def __init__(self, vocab_size, hidden_size=64, lr=1e-2, seed=42):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        self.V = vocab_size\n",
        "        self.H = hidden_size\n",
        "        self.lr = lr\n",
        "\n",
        "        # weights\n",
        "        self.Wxh = rng.normal(0, 0.01, (self.H, self.V))\n",
        "        self.Whh = rng.normal(0, 0.01, (self.H, self.H))\n",
        "        self.Why = rng.normal(0, 0.01, (self.V, self.H))\n",
        "        self.bh = np.zeros((self.H, 1))\n",
        "        self.by = np.zeros((self.V, 1))\n",
        "\n",
        "    def forward(self, inputs, hprev):\n",
        "        \"\"\"\n",
        "        inputs: list of token indices length T\n",
        "        returns cache for backprop\n",
        "        \"\"\"\n",
        "        xs, hs, ys, ps = {}, {}, {}, {}\n",
        "        hs[-1] = hprev\n",
        "\n",
        "        for t, idx in enumerate(inputs):\n",
        "            xs[t] = one_hot(idx, self.V)                         # [V,1]\n",
        "            hs[t] = np.tanh(self.Wxh @ xs[t] + self.Whh @\n",
        "                            hs[t-1] + self.bh)  # [H,1]\n",
        "            ys[t] = self.Why @ hs[t] + self.by                  # [V,1]\n",
        "            ps[t] = softmax(ys[t].ravel()).reshape(-1, 1)       # [V,1]\n",
        "        return xs, hs, ps\n",
        "\n",
        "    def loss_and_grads(self, inputs, targets, hprev):\n",
        "        xs, hs, ps = self.forward(inputs, hprev)\n",
        "\n",
        "        loss = 0.0\n",
        "        for t in range(len(inputs)):\n",
        "            loss += -np.log(ps[t][targets[t], 0] + 1e-12)\n",
        "\n",
        "        # grads init\n",
        "        dWxh = np.zeros_like(self.Wxh)\n",
        "        dWhh = np.zeros_like(self.Whh)\n",
        "        dWhy = np.zeros_like(self.Why)\n",
        "        dbh = np.zeros_like(self.bh)\n",
        "        dby = np.zeros_like(self.by)\n",
        "\n",
        "        dhnext = np.zeros((self.H, 1))\n",
        "\n",
        "        for t in reversed(range(len(inputs))):\n",
        "            dy = ps[t].copy()\n",
        "            # softmax CE gradient\n",
        "            dy[targets[t]] -= 1.0\n",
        "            dWhy += dy @ hs[t].T\n",
        "            dby += dy\n",
        "\n",
        "            dh = self.Why.T @ dy + dhnext\n",
        "            dhraw = (1 - hs[t] * hs[t]) * dh                    # tanh'\n",
        "            dbh += dhraw\n",
        "            dWxh += dhraw @ xs[t].T\n",
        "            dWhh += dhraw @ hs[t-1].T\n",
        "            dhnext = self.Whh.T @ dhraw\n",
        "\n",
        "        # clip\n",
        "        for d in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "            np.clip(d, -5, 5, out=d)\n",
        "\n",
        "        hlast = hs[len(inputs)-1]\n",
        "        return loss, (dWxh, dWhh, dWhy, dbh, dby), hlast\n",
        "\n",
        "    def step(self, grads):\n",
        "        dWxh, dWhh, dWhy, dbh, dby = grads\n",
        "        self.Wxh -= self.lr * dWxh\n",
        "        self.Whh -= self.lr * dWhh\n",
        "        self.Why -= self.lr * dWhy\n",
        "        self.bh -= self.lr * dbh\n",
        "        self.by -= self.lr * dby\n",
        "\n",
        "    def sample(self, start_idx, itos, length=30, temperature=1.0):\n",
        "        h = np.zeros((self.H, 1))\n",
        "        x = one_hot(start_idx, self.V)\n",
        "        out = []\n",
        "\n",
        "        for _ in range(length):\n",
        "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "            y = self.Why @ h + self.by\n",
        "            p = softmax((y.ravel() / max(temperature, 1e-6)))\n",
        "            idx = np.random.choice(range(self.V), p=p)\n",
        "            out.append(itos[idx])\n",
        "            x = one_hot(idx, self.V)\n",
        "        return \" \".join(out)\n",
        "\n",
        "\n",
        "def main():\n",
        "    text = load_poems(\"poems.txt\")\n",
        "    tokens = [\"<bos>\"] + simple_tokenize(text) + [\"<eos>\"]\n",
        "    stoi, itos = build_vocab(tokens, min_freq=1)\n",
        "    ids = tokens_to_ids(tokens, stoi)\n",
        "\n",
        "    rnn = ScratchRNN(vocab_size=len(stoi), hidden_size=128, lr=0.05)\n",
        "    seq_len = 25\n",
        "    h = np.zeros((rnn.H, 1))\n",
        "\n",
        "    # train a bit\n",
        "    for epoch in range(25):\n",
        "        total_loss = 0.0\n",
        "        n = 0\n",
        "        for i in range(0, len(ids) - seq_len - 1, seq_len):\n",
        "            inp = ids[i:i+seq_len]\n",
        "            tgt = ids[i+1:i+seq_len+1]\n",
        "            loss, grads, h = rnn.loss_and_grads(inp, tgt, h)\n",
        "            rnn.step(grads)\n",
        "            total_loss += loss\n",
        "            n += 1\n",
        "\n",
        "        avg = total_loss / max(n, 1)\n",
        "        print(f\"Epoch {epoch+1} | avg loss: {avg:.4f}\")\n",
        "        print(\"Sample:\", rnn.sample(\n",
        "            stoi[\"<bos>\"], itos, length=30, temperature=0.9))\n",
        "        print(\"-\"*80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsIYa9O5oABw",
        "outputId": "2472e5d0-9f14-426b-e1c2-d364463ff80e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | avg loss: 328.6132\n",
            "Sample: something calculation in the this of other , , snow-flakes by to bird the this tardy ' , , could by to and in idle will edge , , to\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2 | avg loss: 270.7710\n",
            "Sample: tops for , love- have in it immigrants , , , spur in in it fancy , , , , in in pursue by , , , swing appeared in\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3 | avg loss: 275.1941\n",
            "Sample: for : straying to it awakes this , the the and to by branches , eager the the majesty and it grave , , confusion the song to birch in\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4 | avg loss: 275.3231\n",
            "Sample: us it mount by follow about things higher resist to it grave , , they barr spirit to it it , name me talk round to it . this ,\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5 | avg loss: 274.1301\n",
            "Sample: replenish , , , , may to to by by , slender foliage and ' to matron by , when dwelling ' ' in birch by , , , '\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6 | avg loss: 274.6477\n",
            "Sample: for out us give have for by by , , , dreams have told thro’ by this , , each talk elements by by , , , , within to\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7 | avg loss: 471.0568\n",
            "Sample: baptized brave thoughtful untying elder nude appointments palest nude mystical eyelashes hurrahs sought repine confidence sport cloth caste mystical bashful mystical mystical scrolls nude layers gamut mystical oconee relate appointments\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8 | avg loss: 586.2071\n",
            "Sample: affections full-noon thoughtful sounding <bos> gross <pad> <pad> <eos> appointments <unk> hunger eden mystical <unk> caste bobs mockings bush <pad> overcome confidence boatman devour <eos> commanding hod <unk> hankering gross\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9 | avg loss: 585.7498\n",
            "Sample: reclined views <eos> <pad> <eos> <pad> <eos> <unk> <unk> <eos> <pad> <bos> <pad> <bos> <unk> <bos> <eos> <eos> <unk> <pad> <pad> <bos> <unk> <bos> <eos> <unk> <pad> <pad> <pad> <eos>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10 | avg loss: 585.3589\n",
            "Sample: kelson <unk> <eos> <eos> <bos> <pad> <eos> <bos> <bos> <eos> <pad> <unk> <unk> <eos> <unk> <pad> <pad> <pad> <eos> <bos> <eos> <unk> <unk> <eos> <pad> <bos> <unk> <bos> <bos> <pad>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11 | avg loss: 585.0223\n",
            "Sample: contenders <eos> <bos> <unk> <pad> <bos> <eos> <bos> <pad> <pad> <pad> <pad> <eos> <eos> <eos> <bos> <pad> <unk> <pad> <eos> <eos> <bos> <bos> <unk> <bos> <eos> <eos> <bos> <eos> <bos>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12 | avg loss: 585.4849\n",
            "Sample: well-taken <unk> <bos> <eos> <unk> <pad> <unk> <bos> <bos> <pad> <unk> <pad> <eos> <bos> <bos> <unk> <eos> <eos> <eos> <pad> <eos> <bos> <pad> <pad> <unk> <unk> <eos> <eos> <unk> <unk>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13 | avg loss: 584.8867\n",
            "Sample: <bos> <unk> <unk> <eos> <pad> <bos> <bos> <unk> <eos> <pad> <pad> <eos> <bos> <eos> <eos> <eos> <eos> <pad> <eos> <unk> <eos> <unk> <pad> <pad> <unk> <bos> <bos> <pad> <eos> <unk>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14 | avg loss: 582.1020\n",
            "Sample: spurt <pad> <unk> <bos> <bos> <eos> <unk> <eos> <pad> <eos> <eos> <pad> <eos> <pad> <bos> <bos> <eos> <pad> <unk> <eos> <eos> <unk> <eos> <unk> <pad> <eos> <bos> <bos> <eos> <eos>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15 | avg loss: 585.6925\n",
            "Sample: breeding <eos> <unk> <unk> <bos> <pad> <unk> <eos> <pad> <pad> <pad> <unk> <bos> <eos> <eos> <pad> <bos> <eos> <bos> <pad> <pad> <eos> <eos> <bos> <eos> <unk> <bos> <bos> <eos> <unk>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 16 | avg loss: 585.3538\n",
            "Sample: buzz <bos> <pad> <eos> <unk> <pad> <pad> <bos> <unk> <unk> <eos> <bos> <bos> <bos> <unk> <eos> <bos> <unk> <unk> <pad> <bos> <unk> <bos> <bos> <unk> <bos> <unk> <pad> <bos> <bos>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 17 | avg loss: 584.0179\n",
            "Sample: promotions <pad> <eos> <eos> <pad> <eos> <bos> <eos> <pad> <eos> <unk> <unk> <unk> <unk> <pad> <pad> <pad> <bos> <unk> <bos> <eos> <unk> <eos> <unk> <unk> <pad> <bos> <bos> <pad> <unk>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 18 | avg loss: 584.3184\n",
            "Sample: slain <bos> <bos> <pad> <pad> <eos> <eos> <pad> <eos> <eos> <bos> <unk> <unk> <eos> <unk> <pad> <pad> <bos> <eos> <unk> <eos> <eos> <bos> <unk> <unk> <bos> <bos> <unk> <pad> <unk>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 19 | avg loss: 581.3918\n",
            "Sample: <bos> <pad> <bos> <unk> <eos> <pad> <unk> <unk> <bos> <unk> <eos> <unk> <pad> <unk> <eos> <bos> <unk> <eos> <pad> <unk> <unk> <eos> <bos> <bos> <eos> <pad> <unk> <eos> <bos> <bos>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 20 | avg loss: 582.6435\n",
            "Sample: <unk> <bos> <eos> <eos> <bos> <unk> <unk> <bos> <unk> <unk> <unk> <bos> <pad> <pad> <eos> <pad> <bos> <eos> <unk> <pad> <unk> <bos> <bos> <bos> <eos> <bos> <eos> <pad> <pad> <pad>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 21 | avg loss: 586.3850\n",
            "Sample: orbit <unk> <bos> <bos> <pad> <bos> <bos> <eos> <eos> <unk> <eos> <pad> <eos> <pad> <bos> <unk> <eos> <eos> <bos> <pad> <pad> <pad> <bos> <pad> <unk> <unk> <unk> <unk> <pad> <pad>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 22 | avg loss: 584.4445\n",
            "Sample: <bos> <eos> <unk> <bos> <bos> <pad> <unk> <eos> <eos> <unk> <unk> <pad> <pad> <pad> <eos> <eos> <unk> <eos> <bos> <unk> <pad> <unk> <unk> <unk> <eos> <eos> <pad> <bos> <eos> <pad>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 23 | avg loss: 585.1448\n",
            "Sample: <eos> <bos> <eos> <pad> <eos> <bos> <pad> <pad> <unk> <eos> <eos> <eos> <bos> <pad> <unk> <eos> <eos> <pad> <bos> <pad> <pad> <pad> <eos> <unk> <pad> <pad> <pad> <unk> <pad> <unk>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 24 | avg loss: 586.4365\n",
            "Sample: <unk> <unk> <eos> <bos> <eos> <bos> <bos> <pad> <bos> <eos> <pad> <bos> <unk> <pad> <pad> <eos> <eos> <bos> <bos> <unk> <pad> <pad> <pad> <bos> <eos> <bos> <unk> <bos> <pad> <bos>\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 25 | avg loss: 585.5850\n",
            "Sample: <unk> <unk> <pad> <bos> <unk> <bos> <bos> <bos> <eos> <eos> <eos> <pad> <pad> <bos> <pad> <unk> <eos> <unk> <pad> <pad> <pad> <bos> <eos> <pad> <unk> <eos> <pad> <eos> <unk> <unk>\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}